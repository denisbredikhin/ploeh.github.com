---
layout: post
title: "Appeal to aithority"
description: "No, it's not a typo."
date: 2025-03-10 14:40 UTC
tags: [Miscellaneous]
---
{% include JB/setup %}

<div id="post">
    <p>
        <em>{{ page.description }}</em>
    </p>
    <p>
        A few months ago, I was listening to a semi-serious programme from the <a href="https://en.wikipedia.org/wiki/DR_P1">Danish public service radio</a>. This is a weekly programme about language that I always listen to as a podcast. The host is the backbone of the show, but in addition to new guests each week, he's flanked by a regular expert who is highly qualified to answer questions about etymology, grammar, semantics, etc.
    </p>
    <p>
        In the episode I'm describing, the expert got a question that a listener had previously emailed. To answer, (s)he started like this (and I'm paraphrasing): <em>I don't actually know the answer to this question, so I did what everyone does these days, when they don't know the answer: I asked ChatGPT.</em>
    </p>
    <p>
        (S)he then proceeded to read aloud what ChatGPT had answered, and concluded with some remarks along the lines that that answer sounded quite plausible.
    </p>
    <p>
        If I used ten to twenty hours of my time re-listening to every episode from the past few months, I could find the particular episode, link to it, transcribe the exact words, and translate them to English to the best of my abilities. I am, however, not going to do that. First, I'm not inclined to use that much time writing an essay on which I make no income. Second, my aim is not to point fingers at anyone in particular, so I'm being deliberately vague. As you may have noticed, I've even masked the person's sex. Not because I don't remember, but to avoid singling out anyone.
    </p>
    <p>
        The expert in question is a regular of the programme, and I've heard him or her give good and knowledgeable answers to many tricky questions. As far as I could tell, this particular question was unanswerable, along the lines of <em>why is 'table' called 'table' rather than 'griungth'?</em>
    </p>
    <p>
        The correct answer would have been <em>I don't know, and I don't think anyone else does.</em>
    </p>
    <p>
        Being a veteran of the programme, (s)he must have realized on beforehand that this wouldn't be good radio, and instead decided to keep it light-hearted.
    </p>
    <p>
        I get that, and I wouldn't be writing about it now if it doesn't look like an example of an increasing trend.
    </p>
    <p>
        People are using large language models (LLMs) to advocate for their positions.
    </p>
    <h3 id="27bb7d65cc8746f09d92f650f0a612eb">
        Appeal to authority <a href="#27bb7d65cc8746f09d92f650f0a612eb">#</a>
    </h3>
    <p>
        <a href="https://en.wikipedia.org/wiki/Argument_from_authority">Appeal to authority</a> is no new technique in discourse.
    </p>
    <blockquote>
        <p>
            "You may also, should it be necessary, not only twist your authorities, but actually falsify them, or quote something which you have invented entirely yourself. As a rule, your opponent has no books at hand, and could not use them if he had."
        </p>
        <footer><cite><a href="https://en.wikipedia.org/wiki/The_Art_of_Being_Right">The Art of Being Right</a></cite>, <a href="https://en.wikipedia.org/wiki/Arthur_Schopenhauer">Arthur Schopenhauer</a>, 1831</footer>
    </blockquote>
    <p>
        This seems similar to how people have begun using so-called artificial intelligence (AI) to do their arguing for them. We may, instead, call this <em>appeal to aithority</em>.
    </p>
    <h3 id="535b5c4b352241f9bfd089677757b18f">
        Epistemological cul-de-sac <a href="#535b5c4b352241f9bfd089677757b18f">#</a>
    </h3>
    <p>
        We've all seen plenty of examples of LLMs being wrong. I'm not going to tire you with any of those here, but I did outline <a href="/2022/12/05/github-copilot-preliminary-experience-report">my experience with GitHub Copilot in 2022</a>. While these technologies may have made some advances since then, they still make basic mistakes.
    </p>
    <p>
        Not only that. They're also non-deterministic. Ask a system a question once, and you get one answer. Ask the same question later, and you may get a variation of the same answer, or perhaps even a contradictory answer. If someone exhibits an answer they got from an LLM as an argument in their favour, consider that they may have been asking it five or six times before they received an answer they liked.
    </p>
    <p>
        Finally, you can easily ask leading questions. Even if someone shows you a screen shot of a chat with an LLM, they may have clipped prior instructions that nudge the system towards a particular bias.
    </p>
    <p>
        I've seen people post screen shots that an LLM claims that <a href="https://fsharp.org/">F#</a> is a better programming language than C#. While I'm sympathetic to that claim, that's not an argument. Just like <a href="/2020/10/12/subjectivity">how you feel about something isn't an argument</a>.
    </p>
    <p>
        This phenomenon seems to be a new trend. People use answers from LLMs as evidence that they are right. I consider this an epistemological dead end.
    </p>
    <h3 id="3a509e32ddc74ecb8dc0c7bf8048156a">
        Real authority <a href="#3a509e32ddc74ecb8dc0c7bf8048156a">#</a>
    </h3>
    <p>
        Regular readers of this blog may have noticed that I often go to great lengths to track down appropriate sources to cite. I do this for several reasons. One is simply out of respect for the people who figured out things before us. Another reason is to strengthen my own arguments.
    </p>
    <p>
        It may seem that I, too, appeal to authority. Indeed, I do. When not used in in the way Schopenhauer describes, citing authority is a necessary epistemological shortcut. If someone who knows much about a particular subject has reached a conclusion based on his or her work, we may (tentatively) accept the conclusion without going through all the same work. As Carl Sagan said, "If you wish to make an apple pie from scratch, you must first invent the universe." You can't do <em>all</em> basic research by yourself. At some point, you'll have to take expert assertions at face value, because you don't have the time, the education, or the money to build your own <a href="https://en.wikipedia.org/wiki/Large_Hadron_Collider">Large Hadron Collider</a>.
    </p>
    <p>
        Don't blindly accept an argument on the only grounds that someone famous said something, but on the other hand, there's no reason to dismiss out of hand what <a href="https://en.wikipedia.org/wiki/Albert_Einstein">Albert Einstein</a> had to say about gravity, just because you've heard that you shouldn't accept an argument based on appeal to authority.
    </p>
    <h3 id="8dbb3f507d8d49b2aa2f322d80fb4031">
        Conclusion <a href="#8dbb3f507d8d49b2aa2f322d80fb4031">#</a>
    </h3>
    <p>
        I'm concerned that people increasingly seem to resort to LLMs to argue a case. The LLMs says this, so it must be right.
    </p>
    <p>
        Sometimes, people will follow up their arguments with <em>of course, it's just an AI, but...</em> and then proceed to unfold their preferred argument. Even if this seems as though the person is making a 'real' argument, starting from an LLM answer establishes a baseline to a discussion. It still lends an aura of truth to something that may be false.
    </p>
</div>